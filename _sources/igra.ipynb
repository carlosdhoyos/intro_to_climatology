{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a1af5a",
   "metadata": {},
   "source": [
    "### IGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32b29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, zipfile, io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c0c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to IGRA2 station list\n",
    "station_list_path = \"./data/IGRA/igra2-station-list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc311aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['name1', 'name2', 'name3'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m stations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_fwf(\n\u001b[1;32m     12\u001b[0m     station_list_path,\n\u001b[1;32m     13\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melev_m\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatei\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatef\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# The station name may be split across multiple columns, so join them\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m stations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stations[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname3\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Keep only relevant columns\u001b[39;00m\n\u001b[1;32m     20\u001b[0m stations \u001b[38;5;241m=\u001b[39m stations[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melev_m\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/jupyterbook_env/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/jupyterbook_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/jupyterbook_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['name1', 'name2', 'name3'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read as whitespace-delimited file\n",
    "# stations = pd.read_csv(\n",
    "#     station_list_path,\n",
    "#     delim_whitespace=True,\n",
    "#     header=None,\n",
    "#     usecols=[0,1,2,3,4,5,6],\n",
    "#     names=[\"id\", \"lat\", \"lon\", \"elev_m\", \"name1\", \"name2\", \"name3\"],\n",
    "#     engine=\"python\"\n",
    "# )\n",
    "\n",
    "stations = pd.read_fwf(\n",
    "    station_list_path,\n",
    "    header=None,\n",
    "    names=['id','lat','lon','elev_m','name','datei','datef','records'])\n",
    "\n",
    "# The station name may be split across multiple columns, so join them\n",
    "stations[\"name\"] = stations[[\"name1\",\"name2\",\"name3\"]].fillna(\"\").agg(\" \".join, axis=1).str.strip()\n",
    "\n",
    "# Keep only relevant columns\n",
    "stations = stations[[\"id\",\"lat\",\"lon\",\"elev_m\",\"name\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter MJO box: 65E–120E, 10S–10N\n",
    "mjo_stations = stations[\n",
    "    (stations[\"lon\"] >= 65) & (stations[\"lon\"] <= 160) &\n",
    "    (stations[\"lat\"] >= -5) & (stations[\"lat\"] <= 5)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "mjo_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose you've already built mjo_stations\n",
    "station = mjo_stations.iloc[0]\n",
    "station_id = station[\"id\"]\n",
    "\n",
    "print(\"Downloading:\", station_id, station[\"name\"])\n",
    "\n",
    "# Build URL\n",
    "url = f\"https://www.ncei.noaa.gov/pub/data/igra/data/data-por/{station_id}-data.txt.zip\"\n",
    "print(url)\n",
    "\n",
    "# Download and unzip\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "fname = z.namelist()[0]\n",
    "\n",
    "# Read lines\n",
    "lines = z.read(fname).decode(\"utf-8\").splitlines()\n",
    "print(\"Number of lines:\", len(lines))\n",
    "print(\"First few lines:\\n\", \"\\n\".join(lines[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f60fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "QC_LETTER_RE = re.compile(r'[A-Za-z]')\n",
    "\n",
    "def _to_num(token: str, scale=1.0, missing=-9999, allow_signed=True):\n",
    "    if token is None:\n",
    "        return np.nan\n",
    "    t = QC_LETTER_RE.sub('', token.strip())\n",
    "    if t == '':\n",
    "        return np.nan\n",
    "    try:\n",
    "        val = int(t) if allow_signed else int(t.replace('-', ''))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    if val == missing:\n",
    "        return np.nan\n",
    "    return val / scale\n",
    "\n",
    "def parse_header(header_line: str):\n",
    "    stn_id   = header_line[1:12].strip()\n",
    "    year     = int(header_line[13:17])\n",
    "    month    = int(header_line[18:20])\n",
    "    day      = int(header_line[21:23])\n",
    "    hour     = int(header_line[24:26])\n",
    "    ts = pd.Timestamp(datetime(year, month, day, hour))\n",
    "    try:\n",
    "        approx_levels = int(header_line[27:31].strip())\n",
    "    except Exception:\n",
    "        approx_levels = None\n",
    "    return stn_id, ts, approx_levels\n",
    "\n",
    "def parse_level_line(ln: str) -> dict:\n",
    "   return {\n",
    "       \"level_type1\": int(ln[0]),\n",
    "       \"level_type2\": int(ln[1]),\n",
    "       \"pressure_hPa\": _to_num(ln[9:15], scale=100.0, missing=-9999),\n",
    "       \"height_m\": _to_num(ln[16:21], scale=1.0, missing=-9999),\n",
    "       \"temp_C\": _to_num(ln[22:27], scale=10.0, missing=-9999),\n",
    "       \"rh_pct\": _to_num(ln[28:33], scale=10.0, missing=-9999),\n",
    "       \"dewpoint_C\": _to_num(ln[34:39], scale=10.0, missing=-9999),\n",
    "       \"wind_dir_deg\": _to_num(ln[40:45], scale=1.0, missing=-9999),\n",
    "       \"wind_speed_mps\": _to_num(ln[46:51], scale=10.0, missing=-9999),\n",
    "   }\n",
    "\n",
    "def split_into_soundings(lines: list[str]):\n",
    "    soundings = []\n",
    "    cur_header, cur_levels = None, []\n",
    "    def flush():\n",
    "        nonlocal cur_header, cur_levels\n",
    "        if cur_header is not None and cur_levels:\n",
    "            soundings.append((cur_header, cur_levels))\n",
    "        cur_header, cur_levels = None, []\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            continue\n",
    "        if ln.startswith('#'):\n",
    "            flush()\n",
    "            cur_header = ln\n",
    "        else:\n",
    "            cur_levels.append(ln)\n",
    "    flush()\n",
    "    return soundings\n",
    "\n",
    "def build_catalog_and_metadata(lines: list[str], drop_empty: bool = True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      catalog: dict[catalog_key -> DataFrame]\n",
    "      meta:    DataFrame with per-sounding stats (includes 'catalog_key' and 'status')\n",
    "    If drop_empty=True, both outputs exclude soundings classified as 'empty'.\n",
    "    \"\"\"\n",
    "    soundings = split_into_soundings(lines)\n",
    "    catalog = {}\n",
    "    meta_rows = []\n",
    "\n",
    "    # handle duplicate timestamps by suffixing keys\n",
    "    duplicates_count: dict[pd.Timestamp, int] = {}\n",
    "\n",
    "    for header, level_lines in soundings:\n",
    "        stn_id, ts, approx_count = parse_header(header)\n",
    "        rows = [parse_level_line(ln) for ln in level_lines]\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.insert(0, \"station\", stn_id)\n",
    "        df.insert(1, \"time\", ts)\n",
    "\n",
    "        # classify BEFORE dropping anything\n",
    "        n_valid_pres = int(df[\"pressure_hPa\"].notna().sum())\n",
    "        n_valid_temp = int(df[\"temp_C\"].notna().sum())\n",
    "        n_valid_wind = int(df[\"wind_speed_mps\"].notna().sum())\n",
    "\n",
    "        if n_valid_pres == 0 and n_valid_temp == 0 and n_valid_wind == 0:\n",
    "            status = \"empty\"\n",
    "        elif n_valid_pres == 0 and n_valid_temp == 0 and n_valid_wind > 0:\n",
    "            status = \"winds_only\"\n",
    "        elif n_valid_temp == 0 and n_valid_pres > 0:\n",
    "            status = \"pressure_only\"\n",
    "        else:\n",
    "            status = \"full\"\n",
    "\n",
    "        # now drop rows that are completely useless across all key fields\n",
    "        df = df.dropna(\n",
    "            how=\"all\",\n",
    "            subset=[\"pressure_hPa\",\"height_m\",\"temp_C\",\"dewpoint_C\",\"wind_dir_deg\",\"wind_speed_mps\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # sort by pressure if present; else by height if present\n",
    "        if df[\"pressure_hPa\"].notna().any():\n",
    "            df = df.sort_values(\"pressure_hPa\", ascending=False).reset_index(drop=True)\n",
    "        elif df[\"height_m\"].notna().any():\n",
    "            df = df.sort_values(\"height_m\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # unique catalog key (timestamp + optional suffix)\n",
    "        if ts in duplicates_count:\n",
    "            duplicates_count[ts] += 1\n",
    "            key = f\"{ts.isoformat()}.{duplicates_count[ts]}\"\n",
    "        else:\n",
    "            duplicates_count[ts] = 1\n",
    "            key = ts.isoformat()\n",
    "\n",
    "        # store even if empty for now; we'll filter at the end if requested\n",
    "        catalog[key] = df\n",
    "\n",
    "        # compute simple stats from the (lightly) cleaned df\n",
    "        n_levels = len(df)\n",
    "        p_max = float(df[\"pressure_hPa\"].max()) if df[\"pressure_hPa\"].notna().any() else np.nan\n",
    "        p_min = float(df[\"pressure_hPa\"].min()) if df[\"pressure_hPa\"].notna().any() else np.nan\n",
    "        z_top = float(df[\"height_m\"].max()) if df[\"height_m\"].notna().any() else np.nan\n",
    "\n",
    "        meta_rows.append({\n",
    "            \"catalog_key\": key,\n",
    "            \"station\": stn_id,\n",
    "            \"time\": ts,\n",
    "            \"status\": status,\n",
    "            \"approx_levels_header\": approx_count,\n",
    "            \"n_levels\": n_levels,\n",
    "            \"n_valid_pressure\": n_valid_pres,\n",
    "            \"n_valid_temp\": n_valid_temp,\n",
    "            \"n_valid_wind\": n_valid_wind,\n",
    "            \"p_max_hPa_surface\": p_max,\n",
    "            \"p_min_hPa_top\": p_min,\n",
    "            \"z_top_m\": z_top,\n",
    "        })\n",
    "\n",
    "    meta = pd.DataFrame(meta_rows).sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    if drop_empty:\n",
    "        # keep only non-empty in meta\n",
    "        meta = meta[meta[\"status\"] != \"empty\"].reset_index(drop=True)\n",
    "        # align catalog to filtered meta\n",
    "        keep_keys = set(meta[\"catalog_key\"])\n",
    "        catalog = {k: v for k, v in catalog.items() if k in keep_keys}\n",
    "\n",
    "    return catalog, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "catalog, meta = build_catalog_and_metadata(lines, drop_empty=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta[\"status\"].value_counts())\n",
    "print(f\"Catalog size: {len(catalog)}  |  Meta rows: {len(meta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95533681",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(catalog), \"soundings parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter meta for full soundings only\n",
    "meta_full = meta[meta[\"status\"] == \"full\"].reset_index(drop=True)\n",
    "\n",
    "# Build a new catalog with only those keys\n",
    "catalog_full = {k: catalog[k] for k in meta_full[\"catalog_key\"]}\n",
    "\n",
    "print(f\"Original catalog: {len(catalog)} soundings\")\n",
    "print(f\"Full-only subset: {len(catalog_full)} soundings\")\n",
    "\n",
    "# Quick check\n",
    "#print(meta_full.head())\n",
    "example_key = meta_full.loc[0, \"catalog_key\"]\n",
    "#print(catalog_full[example_key].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Pick one sounding (first entry in the dict)\n",
    "key, df = next(iter(catalog_full.items()))\n",
    "\n",
    "# df has columns like [\"station\",\"time\",\"level_type\",\"pressure_hPa\",\"height_m\",\"temp_C\",...]\n",
    "\n",
    "# Make a temperature vs. pressure plot (log-y axis, inverted so surface is at bottom)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"temp_C\",\n",
    "    y=\"pressure_hPa\",\n",
    "    markers=True,\n",
    "    title=f\"Sounding at {key} (Station {df['station'].iloc[0]})\",\n",
    "    labels={\"temp_C\": \"Temperature (°C)\", \"pressure_hPa\": \"Pressure (hPa)\"}\n",
    ")\n",
    "\n",
    "# Reverse y-axis so surface (1000 hPa) is at bottom\n",
    "fig.update_yaxes(autorange=\"reversed\", type=\"log\", range=[3, 3])  # auto log scale\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=5),  # make vertical stretch adjustable\n",
    "    width=500,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44ceb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011771cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30099183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b02cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
